#!/usr/bin/env python3
"""
Optimized Issues Router for 1 Lakh+ Concurrent Users

This router provides enterprise-grade issue management with:

- Intelligent caching with Redis cluster
- Batch operations for efficiency
- Advanced query optimization
- Rate limiting per endpoint
- Async processing for heavy operations
- Circuit breakers for fault tolerance
- Performance monitoring
"""

import asyncio
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import json
import hashlib

# FastAPI imports
from fastapi import APIRouter, HTTPException, Depends, Query, BackgroundTasks, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator

# Services
from services.mongodb_optimized_service import get_optimized_mongodb_service
from services.redis_cluster_service import get_redis_cluster_service
from services.ai_service import get_ai_service
from services.email_service import get_email_service
from services.geocode_service import get_geocode_service

# Utilities
from utils.validators import validate_email, validate_zip_code
from utils.helpers import generate_report_id, get_authorities_by_zip_code

logger = logging.getLogger(__name__)

# Create optimized router
router = APIRouter()

# Pydantic models for request/response
class IssueCreate(BaseModel):
    """Model for creating a new issue."""
    address: str = Field(..., min_length=10, max_length=500, description="Issue address")
    zip_code: str = Field(..., min_length=5, max_length=10, description="ZIP code")
    latitude: float = Field(..., ge=-90, le=90, description="Latitude coordinate")
    longitude: float = Field(..., ge=-180, le=180, description="Longitude coordinate")
    issue_type: str = Field(..., min_length=3, max_length=100, description="Type of issue")
    severity: str = Field(..., regex="^(low|medium|high|critical)$", description="Issue severity")
    description: str = Field(..., min_length=10, max_length=2000, description="Issue description")
    user_email: str = Field(..., description="Reporter email")
    phone_number: Optional[str] = Field(None, max_length=15, description="Reporter phone")
    images: Optional[List[str]] = Field(default=[], description="Image URLs")
    category: Optional[str] = Field("general", max_length=50, description="Issue category")
    priority: Optional[str] = Field("normal", regex="^(low|normal|high|urgent)$", description="Issue priority")
    
    @validator('user_email')
    def validate_email_format(cls, v):
        if not validate_email(v):
            raise ValueError('Invalid email format')
        return v.lower()
    
    @validator('zip_code')
    def validate_zip_format(cls, v):
        if not validate_zip_code(v):
            raise ValueError('Invalid ZIP code format')
        return v

class IssueResponse(BaseModel):
    """Model for issue response."""
    id: str
    address: str
    zip_code: str
    latitude: float
    longitude: float
    issue_type: str
    severity: str
    description: str
    status: str
    user_email: str
    phone_number: Optional[str]
    images: List[str]
    category: str
    priority: str
    report_id: str
    created_at: datetime
    updated_at: Optional[datetime]
    ai_analysis: Optional[Dict[str, Any]]
    estimated_resolution_time: Optional[str]

class IssueUpdate(BaseModel):
    """Model for updating an issue."""
    status: Optional[str] = Field(None, regex="^(open|in_progress|resolved|closed)$")
    severity: Optional[str] = Field(None, regex="^(low|medium|high|critical)$")
    priority: Optional[str] = Field(None, regex="^(low|normal|high|urgent)$")
    description: Optional[str] = Field(None, min_length=10, max_length=2000)
    resolution_notes: Optional[str] = Field(None, max_length=1000)
    estimated_resolution_time: Optional[str] = Field(None, max_length=50)

class BulkIssueCreate(BaseModel):
    """Model for bulk issue creation."""
    issues: List[IssueCreate] = Field(..., min_items=1, max_items=100, description="List of issues to create")
    batch_id: Optional[str] = Field(None, description="Batch identifier")

class IssueFilter(BaseModel):
    """Model for filtering issues."""
    status: Optional[str] = None
    severity: Optional[str] = None
    issue_type: Optional[str] = None
    zip_code: Optional[str] = None
    category: Optional[str] = None
    priority: Optional[str] = None
    date_from: Optional[datetime] = None
    date_to: Optional[datetime] = None
    user_email: Optional[str] = None

# Cache key generators
def generate_cache_key(prefix: str, **kwargs) -> str:
    """Generate consistent cache key from parameters."""
    key_data = json.dumps(kwargs, sort_keys=True, default=str)
    key_hash = hashlib.md5(key_data.encode()).hexdigest()[:12]
    return f"{prefix}:{key_hash}"

def generate_list_cache_key(filters: Dict, page: int, limit: int) -> str:
    """Generate cache key for list queries."""
    return generate_cache_key("issues_list", filters=filters, page=page, limit=limit)

# Dependency for rate limiting per endpoint
async def rate_limit_dependency(request: Request):
    """Rate limiting dependency for individual endpoints."""
    # This can be enhanced with more sophisticated rate limiting
    pass

# Background task for heavy operations
async def process_issue_background(issue_data: Dict[str, Any], issue_id: str):
    """
    Background task for processing heavy operations like AI analysis and notifications.
    """
    try:
        logger.info(f"üîÑ Processing background tasks for issue {issue_id}")
        
        # AI analysis (non-blocking)
        ai_service = get_ai_service()
        if ai_service:
            try:
                ai_analysis = await ai_service.analyze_issue(
                    issue_data['description'],
                    issue_data['issue_type'],
                    issue_data['severity']
                )
                
                # Update issue with AI analysis
                mongodb_service = await get_optimized_mongodb_service()
                if mongodb_service:
                    await mongodb_service.update_one_optimized(
                        'issues',
                        {'_id': issue_id},
                        {'$set': {'ai_analysis': ai_analysis, 'ai_processed_at': datetime.utcnow()}}
                    )
                
                logger.info(f"‚úÖ AI analysis completed for issue {issue_id}")
                
            except Exception as e:
                logger.error(f"‚ùå AI analysis failed for issue {issue_id}: {str(e)}")
        
        # Send notifications to authorities (non-blocking)
        try:
            authorities = await get_authorities_by_zip_code(issue_data['zip_code'])
            if authorities:
                email_service = get_email_service()
                if email_service:
                    await email_service.send_issue_notification(
                        authorities,
                        issue_data,
                        issue_id
                    )
                
                logger.info(f"‚úÖ Notifications sent for issue {issue_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Notification failed for issue {issue_id}: {str(e)}")
        
        # Invalidate related caches
        redis_service = await get_redis_cluster_service()
        if redis_service:
            await redis_service.invalidate_pattern("issues_list:*")
            await redis_service.invalidate_pattern(f"issue:{issue_id}")
        
        logger.info(f"‚úÖ Background processing completed for issue {issue_id}")
        
    except Exception as e:
        logger.error(f"‚ùå Background processing failed for issue {issue_id}: {str(e)}")

# Optimized endpoints
@router.post("/issues", response_model=IssueResponse, status_code=201)
async def create_issue(
    issue: IssueCreate,
    background_tasks: BackgroundTasks,
    request: Request,
    _: None = Depends(rate_limit_dependency)
):
    """
    Create a new issue with optimized processing.
    
    This endpoint:
    - Validates input data
    - Stores issue in MongoDB with optimized indexing
    - Generates unique report ID
    - Processes AI analysis in background
    - Sends notifications asynchronously
    - Caches result for future queries
    """
    start_time = time.time()
    
    try:
        # Get MongoDB service
        mongodb_service = await get_optimized_mongodb_service()
        if not mongodb_service:
            raise HTTPException(status_code=503, detail="Database service unavailable")
        
        # Generate unique report ID
        report_id = generate_report_id()
        
        # Prepare issue document
        issue_doc = {
            **issue.dict(),
            'report_id': report_id,
            'status': 'open',
            'created_at': datetime.utcnow(),
            'updated_at': datetime.utcnow(),
            'ai_analysis': None,
            'ai_processed_at': None,
            'estimated_resolution_time': None
        }
        
        # Insert issue into database
        issue_id = await mongodb_service.insert_one_optimized('issues', issue_doc)
        
        # Add background processing
        background_tasks.add_task(process_issue_background, issue_doc, issue_id)
        
        # Prepare response
        response_data = {
            'id': issue_id,
            **issue_doc
        }
        
        # Cache the created issue
        redis_service = await get_redis_cluster_service()
        if redis_service:
            cache_key = f"issue:{issue_id}"
            await redis_service.set_cache('api_response', cache_key, response_data, 3600)
        
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Issue created successfully: {issue_id} in {processing_time:.2f}ms")
        
        return IssueResponse(**response_data)
        
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Failed to create issue after {processing_time:.2f}ms: {str(e)}")
        
        if isinstance(e, HTTPException):
            raise e
        
        raise HTTPException(status_code=500, detail=f"Failed to create issue: {str(e)}")

@router.post("/issues/bulk", status_code=201)
async def create_bulk_issues(
    bulk_request: BulkIssueCreate,
    background_tasks: BackgroundTasks,
    request: Request,
    _: None = Depends(rate_limit_dependency)
):
    """
    Create multiple issues in optimized batches.
    
    This endpoint:
    - Validates all issues in batch
    - Uses MongoDB bulk operations for efficiency
    - Processes each issue in background
    - Returns batch results with individual status
    """
    start_time = time.time()
    
    try:
        # Get MongoDB service
        mongodb_service = await get_optimized_mongodb_service()
        if not mongodb_service:
            raise HTTPException(status_code=503, detail="Database service unavailable")
        
        # Prepare batch documents
        batch_docs = []
        batch_id = bulk_request.batch_id or f"batch_{int(time.time())}"
        
        for issue in bulk_request.issues:
            report_id = generate_report_id()
            issue_doc = {
                **issue.dict(),
                'report_id': report_id,
                'status': 'open',
                'batch_id': batch_id,
                'created_at': datetime.utcnow(),
                'updated_at': datetime.utcnow(),
                'ai_analysis': None,
                'ai_processed_at': None,
                'estimated_resolution_time': None
            }
            batch_docs.append(issue_doc)
        
        # Bulk insert with optimized batching
        inserted_ids = await mongodb_service.insert_many_batch('issues', batch_docs, batch_size=100)
        
        # Add background processing for each issue
        for i, issue_id in enumerate(inserted_ids):
            if i < len(batch_docs):
                background_tasks.add_task(process_issue_background, batch_docs[i], issue_id)
        
        # Invalidate list caches
        redis_service = await get_redis_cluster_service()
        if redis_service:
            await redis_service.invalidate_pattern("issues_list:*")
        
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Bulk created {len(inserted_ids)} issues in {processing_time:.2f}ms")
        
        return {
            'message': f'Successfully created {len(inserted_ids)} issues',
            'batch_id': batch_id,
            'created_count': len(inserted_ids),
            'issue_ids': inserted_ids,
            'processing_time_ms': round(processing_time, 2)
        }
        
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Bulk create failed after {processing_time:.2f}ms: {str(e)}")
        
        if isinstance(e, HTTPException):
            raise e
        
        raise HTTPException(status_code=500, detail=f"Bulk create failed: {str(e)}")

@router.get("/issues", response_model=List[IssueResponse])
async def get_issues(
    request: Request,
    page: int = Query(1, ge=1, description="Page number"),
    limit: int = Query(20, ge=1, le=100, description="Items per page"),
    status: Optional[str] = Query(None, description="Filter by status"),
    severity: Optional[str] = Query(None, description="Filter by severity"),
    issue_type: Optional[str] = Query(None, description="Filter by issue type"),
    zip_code: Optional[str] = Query(None, description="Filter by ZIP code"),
    category: Optional[str] = Query(None, description="Filter by category"),
    priority: Optional[str] = Query(None, description="Filter by priority"),
    user_email: Optional[str] = Query(None, description="Filter by user email"),
    _: None = Depends(rate_limit_dependency)
):
    """
    Get paginated list of issues with intelligent caching.
    
    This endpoint:
    - Supports multiple filters
    - Uses intelligent caching with Redis
    - Optimized MongoDB queries with indexes
    - Pagination for large datasets
    - Cache invalidation on updates
    """
    start_time = time.time()
    
    try:
        # Build filter dictionary
        filters = {}
        if status:
            filters['status'] = status
        if severity:
            filters['severity'] = severity
        if issue_type:
            filters['issue_type'] = issue_type
        if zip_code:
            filters['zip_code'] = zip_code
        if category:
            filters['category'] = category
        if priority:
            filters['priority'] = priority
        if user_email:
            filters['user_email'] = user_email.lower()
        
        # Generate cache key
        cache_key = generate_list_cache_key(filters, page, limit)
        
        # Get MongoDB service
        mongodb_service = await get_optimized_mongodb_service()
        if not mongodb_service:
            raise HTTPException(status_code=503, detail="Database service unavailable")
        
        # Calculate skip value
        skip = (page - 1) * limit
        
        # Query with caching
        issues = await mongodb_service.find_with_cache(
            collection_name='issues',
            filter_dict=filters,
            cache_key=cache_key,
            cache_ttl=300,  # 5 minutes cache
            limit=limit,
            skip=skip,
            sort=[('created_at', -1)]  # Sort by newest first
        )
        
        # Convert to response models
        response_issues = []
        for issue in issues:
            issue_response = IssueResponse(
                id=issue['_id'],
                **{k: v for k, v in issue.items() if k != '_id'}
            )
            response_issues.append(issue_response)
        
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Retrieved {len(response_issues)} issues in {processing_time:.2f}ms")
        
        return response_issues
        
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Failed to get issues after {processing_time:.2f}ms: {str(e)}")
        
        if isinstance(e, HTTPException):
            raise e
        
        raise HTTPException(status_code=500, detail=f"Failed to retrieve issues: {str(e)}")

@router.get("/issues/{issue_id}", response_model=IssueResponse)
async def get_issue(
    issue_id: str,
    request: Request,
    _: None = Depends(rate_limit_dependency)
):
    """
    Get a specific issue by ID with caching.
    
    This endpoint:
    - Retrieves single issue by ID
    - Uses Redis caching for fast access
    - Handles ObjectId conversion
    - Returns 404 if not found
    """
    start_time = time.time()
    
    try:
        # Get MongoDB service
        mongodb_service = await get_optimized_mongodb_service()
        if not mongodb_service:
            raise HTTPException(status_code=503, detail="Database service unavailable")
        
        # Try cache first
        cache_key = f"issue:{issue_id}"
        redis_service = await get_redis_cluster_service()
        
        if redis_service:
            cached_issue = await redis_service.get_cache('api_response', cache_key)
            if cached_issue:
                processing_time = (time.time() - start_time) * 1000
                logger.info(f"‚úÖ Cache HIT: Retrieved issue {issue_id} in {processing_time:.2f}ms")
                return IssueResponse(**cached_issue)
        
        # Cache miss - query database
        from bson.objectid import ObjectId
        
        try:
            object_id = ObjectId(issue_id)
        except Exception:
            raise HTTPException(status_code=400, detail="Invalid issue ID format")
        
        issues = await mongodb_service.find_with_cache(
            collection_name='issues',
            filter_dict={'_id': object_id},
            cache_key=cache_key,
            cache_ttl=1800,  # 30 minutes cache for individual issues
            limit=1
        )
        
        if not issues:
            raise HTTPException(status_code=404, detail="Issue not found")
        
        issue = issues[0]
        issue_response = IssueResponse(
            id=issue['_id'],
            **{k: v for k, v in issue.items() if k != '_id'}
        )
        
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Retrieved issue {issue_id} in {processing_time:.2f}ms")
        
        return issue_response
        
    except HTTPException:
        raise
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Failed to get issue {issue_id} after {processing_time:.2f}ms: {str(e)}")
        
        raise HTTPException(status_code=500, detail=f"Failed to retrieve issue: {str(e)}")

@router.put("/issues/{issue_id}", response_model=IssueResponse)
async def update_issue(
    issue_id: str,
    issue_update: IssueUpdate,
    background_tasks: BackgroundTasks,
    request: Request,
    _: None = Depends(rate_limit_dependency)
):
    """
    Update an existing issue with cache invalidation.
    
    This endpoint:
    - Updates issue fields
    - Invalidates related caches
    - Tracks update history
    - Sends notifications if status changed
    """
    start_time = time.time()
    
    try:
        # Get MongoDB service
        mongodb_service = await get_optimized_mongodb_service()
        if not mongodb_service:
            raise HTTPException(status_code=503, detail="Database service unavailable")
        
        # Prepare update document
        from bson.objectid import ObjectId
        
        try:
            object_id = ObjectId(issue_id)
        except Exception:
            raise HTTPException(status_code=400, detail="Invalid issue ID format")
        
        # Build update dictionary
        update_data = {k: v for k, v in issue_update.dict().items() if v is not None}
        
        if not update_data:
            raise HTTPException(status_code=400, detail="No update data provided")
        
        update_data['updated_at'] = datetime.utcnow()
        
        # Update issue
        updated = await mongodb_service.update_one_optimized(
            'issues',
            {'_id': object_id},
            {'$set': update_data}
        )
        
        if not updated:
            raise HTTPException(status_code=404, detail="Issue not found")
        
        # Get updated issue
        issues = await mongodb_service.find_with_cache(
            collection_name='issues',
            filter_dict={'_id': object_id},
            cache_key=None,  # Don't cache during update
            limit=1
        )
        
        if not issues:
            raise HTTPException(status_code=404, detail="Issue not found after update")
        
        issue = issues[0]
        
        # Invalidate caches
        redis_service = await get_redis_cluster_service()
        if redis_service:
            await redis_service.invalidate_pattern(f"issue:{issue_id}")
            await redis_service.invalidate_pattern("issues_list:*")
        
        # Add background notification if status changed
        if 'status' in update_data:
            background_tasks.add_task(
                send_status_update_notification,
                issue,
                update_data['status']
            )
        
        issue_response = IssueResponse(
            id=issue['_id'],
            **{k: v for k, v in issue.items() if k != '_id'}
        )
        
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Updated issue {issue_id} in {processing_time:.2f}ms")
        
        return issue_response
        
    except HTTPException:
        raise
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Failed to update issue {issue_id} after {processing_time:.2f}ms: {str(e)}")
        
        raise HTTPException(status_code=500, detail=f"Failed to update issue: {str(e)}")

@router.delete("/issues/{issue_id}", status_code=204)
async def delete_issue(
    issue_id: str,
    request: Request,
    _: None = Depends(rate_limit_dependency)
):
    """
    Delete an issue with cache cleanup.
    
    This endpoint:
    - Soft deletes issue (marks as deleted)
    - Cleans up related caches
    - Logs deletion for audit
    """
    start_time = time.time()
    
    try:
        # Get MongoDB service
        mongodb_service = await get_optimized_mongodb_service()
        if not mongodb_service:
            raise HTTPException(status_code=503, detail="Database service unavailable")
        
        from bson.objectid import ObjectId
        
        try:
            object_id = ObjectId(issue_id)
        except Exception:
            raise HTTPException(status_code=400, detail="Invalid issue ID format")
        
        # Soft delete (mark as deleted instead of removing)
        updated = await mongodb_service.update_one_optimized(
            'issues',
            {'_id': object_id},
            {
                '$set': {
                    'status': 'deleted',
                    'deleted_at': datetime.utcnow(),
                    'updated_at': datetime.utcnow()
                }
            }
        )
        
        if not updated:
            raise HTTPException(status_code=404, detail="Issue not found")
        
        # Clean up caches
        redis_service = await get_redis_cluster_service()
        if redis_service:
            await redis_service.invalidate_pattern(f"issue:{issue_id}")
            await redis_service.invalidate_pattern("issues_list:*")
        
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Deleted issue {issue_id} in {processing_time:.2f}ms")
        
        return None
        
    except HTTPException:
        raise
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Failed to delete issue {issue_id} after {processing_time:.2f}ms: {str(e)}")
        
        raise HTTPException(status_code=500, detail=f"Failed to delete issue: {str(e)}")

# Background task for status update notifications
async def send_status_update_notification(issue_data: Dict[str, Any], new_status: str):
    """
    Send notification when issue status is updated.
    """
    try:
        email_service = get_email_service()
        if email_service and 'user_email' in issue_data:
            await email_service.send_status_update(
                issue_data['user_email'],
                issue_data,
                new_status
            )
            logger.info(f"‚úÖ Status update notification sent for issue {issue_data.get('_id')}")
    except Exception as e:
        logger.error(f"‚ùå Failed to send status update notification: {str(e)}")

# Analytics endpoints
@router.get("/issues/analytics/summary")
async def get_issues_summary(
    request: Request,
    days: int = Query(30, ge=1, le=365, description="Number of days for analytics"),
    _: None = Depends(rate_limit_dependency)
):
    """
    Get issues analytics summary with caching.
    
    This endpoint:
    - Provides aggregated statistics
    - Uses MongoDB aggregation pipeline
    - Caches results for performance
    - Supports different time ranges
    """
    start_time = time.time()
    
    try:
        # Get MongoDB service
        mongodb_service = await get_optimized_mongodb_service()
        if not mongodb_service:
            raise HTTPException(status_code=503, detail="Database service unavailable")
        
        # Calculate date range
        end_date = datetime.utcnow()
        start_date = end_date - timedelta(days=days)
        
        # Generate cache key
        cache_key = generate_cache_key("analytics_summary", days=days, date=end_date.date())
        
        # Aggregation pipeline
        pipeline = [
            {
                '$match': {
                    'created_at': {'$gte': start_date, '$lte': end_date},
                    'status': {'$ne': 'deleted'}
                }
            },
            {
                '$group': {
                    '_id': None,
                    'total_issues': {'$sum': 1},
                    'open_issues': {
                        '$sum': {'$cond': [{'$eq': ['$status', 'open']}, 1, 0]}
                    },
                    'in_progress_issues': {
                        '$sum': {'$cond': [{'$eq': ['$status', 'in_progress']}, 1, 0]}
                    },
                    'resolved_issues': {
                        '$sum': {'$cond': [{'$eq': ['$status', 'resolved']}, 1, 0]}
                    },
                    'closed_issues': {
                        '$sum': {'$cond': [{'$eq': ['$status', 'closed']}, 1, 0]}
                    },
                    'critical_issues': {
                        '$sum': {'$cond': [{'$eq': ['$severity', 'critical']}, 1, 0]}
                    },
                    'high_issues': {
                        '$sum': {'$cond': [{'$eq': ['$severity', 'high']}, 1, 0]}
                    },
                    'medium_issues': {
                        '$sum': {'$cond': [{'$eq': ['$severity', 'medium']}, 1, 0]}
                    },
                    'low_issues': {
                        '$sum': {'$cond': [{'$eq': ['$severity', 'low']}, 1, 0]}
                    }
                }
            }
        ]
        
        # Execute aggregation with caching
        results = await mongodb_service.aggregate_with_cache(
            collection_name='issues',
            pipeline=pipeline,
            cache_key=cache_key,
            cache_ttl=1800  # 30 minutes cache
        )
        
        # Format response
        if results:
            summary = results[0]
            summary.pop('_id', None)
        else:
            summary = {
                'total_issues': 0,
                'open_issues': 0,
                'in_progress_issues': 0,
                'resolved_issues': 0,
                'closed_issues': 0,
                'critical_issues': 0,
                'high_issues': 0,
                'medium_issues': 0,
                'low_issues': 0
            }
        
        # Add metadata
        summary.update({
            'date_range': {
                'start_date': start_date.isoformat(),
                'end_date': end_date.isoformat(),
                'days': days
            },
            'generated_at': datetime.utcnow().isoformat()
        })
        
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"‚úÖ Generated analytics summary in {processing_time:.2f}ms")
        
        return summary
        
    except Exception as e:
        processing_time = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Failed to generate analytics after {processing_time:.2f}ms: {str(e)}")
        
        if isinstance(e, HTTPException):
            raise e
        
        raise HTTPException(status_code=500, detail=f"Failed to generate analytics: {str(e)}")
